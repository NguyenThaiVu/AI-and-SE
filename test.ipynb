{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3fb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import javalang\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f5245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not TOKEN:\n",
    "    raise SystemExit(\"‚ùå Please set GITHUB_TOKEN in your environment.\")\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Accept\": \"application/vnd.github+json\"}\n",
    "\n",
    "DATA_DIR = Path(\"java_repos\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3470e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_java_repos(n=5, min_star=50):\n",
    "    \"\"\"\n",
    "    Search Java repos by stars with pagination support.\n",
    "    * Arguments:\n",
    "        - n: number of repos to fetch\n",
    "        - min_star: minimum stars to filter repos\n",
    "    * Returns:\n",
    "        - list of repo dicts as returned by GitHub API\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://api.github.com/search/repositories\"\n",
    "    repos, page = [], 1\n",
    "    while len(repos) < n:\n",
    "        params = {\n",
    "            \"q\": f\"language:Java stars:>{min_star}\",\n",
    "            \"sort\": \"stars\",\n",
    "            \"order\": \"desc\",\n",
    "            \"per_page\": min(100, n - len(repos)),\n",
    "            \"page\": page,\n",
    "        }\n",
    "        r = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        items = r.json().get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        repos.extend(items)\n",
    "        page += 1\n",
    "    return repos[:n]\n",
    "\n",
    "\n",
    "def get_repo_files(owner, repo, branch=\"main\", extension=\".java\"):\n",
    "    \"\"\"\n",
    "    Get all file paths in a repo.\n",
    "    * Arguments:\n",
    "        - owner: repo owner\n",
    "        - repo: repo name\n",
    "        - branch: branch name (default: main)\n",
    "    * Returns:\n",
    "        - list of file paths (strings)\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/{branch}?recursive=1\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    tree = r.json().get(\"tree\", [])\n",
    "    return [item[\"path\"] for item in tree if item[\"type\"] == \"blob\" and item[\"path\"].endswith(extension)]\n",
    "\n",
    "\n",
    "def get_last_commit(owner, repo, filepath):\n",
    "    \"\"\"\n",
    "    Get the last commit SHA for a given file.   \n",
    "    \n",
    "    Since each file can have multiple method, it is hard to track commit per method. \n",
    "    We assume the last commit of the file is the commit for all methods in that file.\n",
    "\n",
    "    * Arguments:\n",
    "        - owner: repo owner\n",
    "        - repo: repo name\n",
    "        - filepath: path to the file in the repo\n",
    "    * Returns:\n",
    "        - commit SHA (string) or None if not found\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    params = {\"path\": filepath, \"per_page\": 1}\n",
    "    r = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    commits = r.json()\n",
    "    return commits[0][\"sha\"] if commits else None\n",
    "\n",
    "\n",
    "def download_and_extract_repo(owner, repo, branch, dest):\n",
    "    \"\"\"Download repo as zipball and extract it locally.\"\"\"\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = dest / f\"{owner}-{repo}-{branch}.zip\"\n",
    "    extract_dir = dest / f\"{owner}-{repo}-{branch}\"\n",
    "\n",
    "    if extract_dir.exists():\n",
    "        shutil.rmtree(extract_dir)\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/zipball/{branch}\"\n",
    "    r = requests.get(url, headers=HEADERS, stream=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(1024 * 256):\n",
    "            f.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "\n",
    "    # GitHub zip includes a single top-level folder, return that\n",
    "    subfolders = list(extract_dir.iterdir())\n",
    "    return subfolders[0] if subfolders else extract_dir\n",
    "\n",
    "\n",
    "def extract_methods_from_file(filepath):\n",
    "    \"\"\"Extract methods from a Java file using javalang.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            code = f.read()\n",
    "        tree = javalang.parse.parse(code)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    lines = code.splitlines()\n",
    "    for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "        method_name = node.name\n",
    "        start_line = node.position.line if node.position else None\n",
    "        end_line = None\n",
    "        if node.body and node.body[-1].position:\n",
    "            end_line = node.body[-1].position.line\n",
    "        signature = f\"{' '.join(node.modifiers)} {node.return_type} {method_name}({', '.join(str(p.type) for p in node.parameters)})\"\n",
    "        original_code = \"\\n\".join(lines[start_line-1:end_line]) if start_line and end_line else \"\"\n",
    "        code_tokens = original_code.split()  # simple whitespace tokenization\n",
    "        results.append({\n",
    "            \"method_name\": method_name,\n",
    "            \"start_line\": start_line,\n",
    "            \"end_line\": end_line,\n",
    "            \"signature\": signature,\n",
    "            \"original_code\": original_code,\n",
    "            \"code_tokens\": code_tokens,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_dataset(n_repos=2, min_star=500, max_files=50, output_csv=\"java_functions_dataset.csv\"):\n",
    "    all_results = []\n",
    "    repos = search_java_repos(n=n_repos, min_star=min_star)\n",
    "\n",
    "    for repo in repos:\n",
    "        owner, name = repo[\"full_name\"].split(\"/\")\n",
    "        branch = repo[\"default_branch\"]\n",
    "        repo_url = repo[\"html_url\"]\n",
    "\n",
    "        print(f\"\\nüîπ Processing repo: {repo['full_name']} (branch={branch})\")\n",
    "\n",
    "        # Get files and commits\n",
    "        files = get_repo_files(owner, name, branch)\n",
    "        print(f\"   Found {len(files)} .java files\")\n",
    "\n",
    "        local_repo = download_and_extract_repo(owner, name, branch, DATA_DIR)\n",
    "\n",
    "        for f in files[:max_files]:  # avoid crawling too many files\n",
    "            commit_sha = get_last_commit(owner, name, f)\n",
    "\n",
    "            file_path = local_repo / f\n",
    "            if not file_path.exists():\n",
    "                continue\n",
    "\n",
    "            methods = extract_methods_from_file(file_path)\n",
    "            for m in methods:\n",
    "                all_results.append({\n",
    "                    \"repo_name\": repo[\"full_name\"],\n",
    "                    \"repo_url\": repo_url,\n",
    "                    \"commit_sha\": commit_sha,\n",
    "                    \"file_path\": f,\n",
    "                    **m\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c91c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_dataset(n_repos=3, min_star=50, max_files=30)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c080170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"methods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c9f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resources': {'core': {'limit': 5000, 'used': 5000, 'remaining': 0, 'reset': 1758148849}, 'search': {'limit': 30, 'used': 0, 'remaining': 30, 'reset': 1758147425}, 'graphql': {'limit': 5000, 'used': 4, 'remaining': 4996, 'reset': 1758147532}, 'integration_manifest': {'limit': 5000, 'used': 0, 'remaining': 5000, 'reset': 1758150965}, 'source_import': {'limit': 100, 'used': 0, 'remaining': 100, 'reset': 1758147425}, 'code_scanning_upload': {'limit': 5000, 'used': 5000, 'remaining': 0, 'reset': 1758148849}, 'code_scanning_autofix': {'limit': 10, 'used': 0, 'remaining': 10, 'reset': 1758147425}, 'actions_runner_registration': {'limit': 10000, 'used': 0, 'remaining': 10000, 'reset': 1758150965}, 'scim': {'limit': 15000, 'used': 0, 'remaining': 15000, 'reset': 1758150965}, 'dependency_snapshots': {'limit': 100, 'used': 0, 'remaining': 100, 'reset': 1758147425}, 'dependency_sbom': {'limit': 100, 'used': 0, 'remaining': 100, 'reset': 1758147425}, 'audit_log': {'limit': 1750, 'used': 0, 'remaining': 1750, 'reset': 1758150965}, 'audit_log_streaming': {'limit': 15, 'used': 0, 'remaining': 15, 'reset': 1758150965}, 'code_search': {'limit': 10, 'used': 0, 'remaining': 10, 'reset': 1758147425}}, 'rate': {'limit': 5000, 'used': 5000, 'remaining': 0, 'reset': 1758148849}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"GITHUB_TOKEN\")  # load from env if set\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "\n",
    "url = \"https://api.github.com/rate_limit\"\n",
    "r = requests.get(url, headers=headers, timeout=30)\n",
    "r.raise_for_status()\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a8289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
